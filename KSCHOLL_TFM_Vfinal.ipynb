{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KSCHOLL TFM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data \n",
    "First of all, load the needed libraries.\n",
    "\n",
    "Geograpy usually give some issues during the installation, better use: \n",
    "`pip install geograpy3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datadotworld as dw\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from dateparser.search import search_dates\n",
    "import geograpy as gt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create conection with the Dataworld API and load the dataset with the URLS of all the datasets from WPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dw.api_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset with all URLs of WPP datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfD = pd.read_excel('https://query.data.world/s/gal5osq7zrw25gm5m265ogeyantsxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load all the metadata from the datasets and save it in a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = client.fetch_contributing_datasets(limit = '100')\n",
    "next_page = datasets.get('next_page_token')\n",
    "ldatasets = datasets.get('records')\n",
    "\n",
    "while next_page != None:\n",
    "    datasets = client.fetch_contributing_datasets(limit = '100', next = next_page)\n",
    "    next_page = datasets.get('next_page_token')\n",
    "    ldatasets = ldatasets + datasets.get('records')\n",
    "    \n",
    "df = pd.DataFrame.from_dict(ldatasets[0], orient='index').transpose()\n",
    "for i in range(1,len(ldatasets)):\n",
    "    df = df.append(pd.DataFrame.from_dict(ldatasets[i], orient='index').transpose())\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start working with the metadata. First, extrat the tags and save it in a new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtagsl = []\n",
    "for index, row in df.iterrows():\n",
    "    for i in row.tags:\n",
    "        dtagsl.append([row.id,i])\n",
    "        \n",
    "dftags = pd.DataFrame(dtagsl, columns = ['Name','Tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary have a lot of info, we are going to extract and distribute it into new cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    s = str(row.summary)\n",
    "    resultC = re.findall('\\*\\*(.*):\\*\\*', s)\n",
    "    resultF = re.findall(':\\*\\*(.*)\\**', s)\n",
    "    n = 0\n",
    "    for i in resultC:\n",
    "        if i in df.columns:\n",
    "            df.loc[index,i] = resultF[n]\n",
    "            n += 1\n",
    "        else:\n",
    "            df[i] = None\n",
    "            df.loc[index,i] = resultF[n]\n",
    "            n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some cols are no representative because most of the are empty. We remove all the cols with less than 80% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = df.isna().sum()\n",
    "n = 0\n",
    "colsToDrop = []\n",
    "for i in nulls:\n",
    "    if i/df.shape[0] > 0.80:\n",
    "        colsToDrop.append(nulls.index[n])\n",
    "        df = df.drop([nulls.index[n]] , axis = 1)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find dates on the summary of each dataset.\n",
    "\n",
    "#### If we find two dates we consider as starting and finish date of the data.\n",
    "\n",
    "#### We save it in a new DF with name-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if df.iloc[index].When != None:\n",
    "        try:\n",
    "            dates = search_dates(df.iloc[index].When.strip())\n",
    "            firstDate = dates[0][1].year\n",
    "            lastDate = dates[len(dates) - 1][1].year\n",
    "            if firstDate < lastDate:\n",
    "                df.loc[index,'MinD'] = firstDate\n",
    "                df.loc[index,'MaxD'] = lastDate\n",
    "            elif firstDate > lastDate:\n",
    "                df.loc[index,'MinD'] = lastDate \n",
    "                df.loc[index,'MaxD'] = firstDate\n",
    "            elif firstDate == lastDate:\n",
    "                df.loc[index,'MinD'] = lastDate \n",
    "                df.loc[index,'MaxD'] = lastDate            \n",
    "        except:\n",
    "            None\n",
    "            \n",
    "ddates = []\n",
    "for index, row in df.iterrows():\n",
    "    if not np.isnan(row.MinD):\n",
    "        if int(row.MinD) == int(row.MaxD):\n",
    "            ddates.append([row.id, int(row.MinD)])            \n",
    "        else:\n",
    "            for i in range(int(row.MinD),int(row.MaxD) + 1):\n",
    "                ddates.append([row.id, i])\n",
    "                \n",
    "dfdates = pd.DataFrame(ddates, columns = ['Name','Year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load an online .csv with country name and compare with the ones that we haave extractec from the summaries.\n",
    "\n",
    "#### Create a new DF with dataset name and dates included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.read_csv('https://raw.githubusercontent.com/datasets/country-list/master/data.csv')\n",
    "\n",
    "dcountries = []\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        places = gt.get_place_context(text = df.iloc[index].summary)\n",
    "        for i in places.country_mentions:\n",
    "            if i[0] in list(countries.Name):\n",
    "                dcountries.append([row.id, i[0]])\n",
    "    except:\n",
    "        None\n",
    "        \n",
    "dfcountries = pd.DataFrame(dcountries, columns = ['Name','Country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new col with the number of files from each dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, rows in df.iterrows():\n",
    "    df.loc[index, 'nfiles'] = len(df.files[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To have a good final DF we need to merge all the df that we have.\n",
    "\n",
    "#### Merge by name doesn't works with all the dataset, we have to merge also by title and manage the duplicated cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'id' : 'Name'})\n",
    "dfF = pd.merge(df,dfD, on = 'Name' , how = 'left')\n",
    "dfF = pd.merge(dfF,dfD, left_on = 'title' , right_on = 'Name', how = 'left')\n",
    "\n",
    "dfF[\"URL_x\"] = dfF[\"URL_x\"].fillna(dfF[\"URL_y\"])\n",
    "dfF[\"Collection_x\"] = dfF[\"Collection_x\"].fillna(dfF[\"Collection_y\"])\n",
    "dfF[\"Topic _x\"] = dfF[\"Topic _x\"].fillna(dfF[\"Topic _y\"])\n",
    "dfF[\"Source Type_x\"] = dfF[\"Source Type_x\"].fillna(dfF[\"Source Type_y\"])\n",
    "dfF[\"Privacy_x\"] = dfF[\"Privacy_x\"].fillna(dfF[\"Privacy_y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have to storage all the data to can work with the visualization, SQL is a good option.\n",
    "\n",
    "#### Import the needed libraries. \n",
    "\n",
    "#### Install progressbar with `pip install progressbar2`\n",
    "\n",
    "#### Install PyMySQL with `pip install PyMySQL``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import progressbar\n",
    "import pymysql\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step y create the tables in the db for all the dataframes and one extra for a data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_tabletag(con):\n",
    "    cursorObj.execute(\"CREATE TABLE dftags(title text , tag text)\")\n",
    "    con.commit()\n",
    "    \n",
    "def sql_tablefull(con):\n",
    "    cursorObj = con.cursor()\n",
    "    cursorObj.execute(\"CREATE TABLE full_data(owner text , Name text , title text, description text, summary text, tags text, visibility text, files text, status text,  created text, updated text, is_project text, access_level text,Data_Origin text, What text, When_ text, AutoSync text, Cite text, Who text, Source_Description text, MinD text, MaxD text, URL text, Collection text, Topic text, SourceType text, Privacy text)\")\n",
    "    con.commit()\n",
    "\n",
    "def sql_tabledates(con):\n",
    "    cursorObj.execute(\"CREATE TABLE dates(title text , date text)\")\n",
    "    con.commit()\n",
    "    \n",
    "def sql_tablecountries(con):\n",
    "    cursorObj.execute(\"CREATE TABLE countries(title text , country text)\")\n",
    "    con.commit()\n",
    "\n",
    "def sql_tablesummary(con):\n",
    "    cursorObj = con.cursor()\n",
    "    cursorObj.execute(\"CREATE TABLE summary(updated text , datasets INT , files INT, cat_sup INT, topics INT, tags INT, countries INT, MinD INT)\")\n",
    "    con.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After have all the tables, we prepare function to upload data to the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertDFtags(df, con):\n",
    "    cursorObj = con.cursor()\n",
    "    #Before add data, delete all the data in the table. Faster than review the existing data.\n",
    "    cursorObj.execute('DELETE FROM dftags;',);\n",
    "    con.commit()\n",
    "\n",
    "    barposition=0\n",
    "    with progressbar.ProgressBar(max_value=len(df)) as bar:\n",
    "        cursorObj = con.cursor()\n",
    "        for i,row in df.iterrows():\n",
    "            arr = df.iloc[i]\n",
    "            insertsql = ('INSERT INTO dftags VALUES'+(\"('\"+arr[0]+\"' , '\"+arr[1]+\"')\"))\n",
    "            cursorObj.execute(insertsql)\n",
    "            con.commit()\n",
    "            barposition+=1\n",
    "            bar.update(i)\n",
    "            \n",
    "#Replace some special characters because it gives some issues when you try to insert into the SQL\n",
    "def insertDFfull(df, con):\n",
    "    cursorObj = con.cursor()\n",
    "    try: \n",
    "        cursorObj.execute('DELETE FROM full_data;',);\n",
    "        con.commit()\n",
    "    except:\n",
    "        pass\n",
    "    barposition=0\n",
    "    with progressbar.ProgressBar(max_value=len(df)) as bar:    \n",
    "        cursorObj = con.cursor()\n",
    "        for i,row in df.iterrows():\n",
    "            arr = df.iloc[i]\n",
    "\n",
    "            title = arr[2]\n",
    "            try:\n",
    "                title = title.replace(\"/\",\"\").replace(\"-\",\"\").replace(\".\",\" \").replace(\",\",\" \").replace(\":\",\"\").replace(\"*\",\"\").replace(\"'\",\"\").replace(\"{\",\"\").replace(\"}\",\"\")\n",
    "            except:\n",
    "                title = str(arr[2])\n",
    "\n",
    "            summary = arr[4]\n",
    "            try:\n",
    "                summary = summary.replace(\"/\",\"\").replace(\"-\",\"\").replace(\".\",\" \").replace(\",\",\" \").replace(\":\",\"\").replace(\"*\",\"\").replace(\"'\",\"\").replace(\"{\",\"\").replace(\"}\",\"\") \n",
    "            except:\n",
    "                summary = str(arr[4])\n",
    "\n",
    "            descrip = arr[3]\n",
    "            try: \n",
    "                descrip = descrip.replace(\"/\",\"\").replace(\"-\",\"\").replace(\".\",\" \").replace(\",\",\" \").replace(\":\",\"\").replace(\"*\",\"\").replace(\"'\",\"\").replace(\"{\",\"\").replace(\"}\",\"\") \n",
    "            except:\n",
    "                descrip = str(arr[3])\n",
    "\n",
    "            source = arr[19]\n",
    "            try: \n",
    "                source = source.replace(\"/\",\"\").replace(\"-\",\"\").replace(\".\",\" \").replace(\",\",\" \").replace(\":\",\"\").replace(\"*\",\"\").replace(\"'\",\"\").replace(\"{\",\"\").replace(\"}\",\"\") \n",
    "            except:\n",
    "                source = str(arr[19])\n",
    "\n",
    "            cite = arr[17]\n",
    "            try: \n",
    "                cite = cite.replace(\"/\",\"\").replace(\"-\",\"\").replace(\".\",\" \").replace(\",\",\" \").replace(\":\",\"\").replace(\"*\",\"\").replace(\"'\",\"\").replace(\"{\",\"\").replace(\"}\",\"\") \n",
    "            except:\n",
    "                cite = str(arr[17])\n",
    "\n",
    "\n",
    "            whatt = arr[14]\n",
    "            try: \n",
    "                whatt = whatt.replace(\"/\",\"\").replace(\"-\",\"\").replace(\".\",\" \").replace(\",\",\" \").replace(\":\",\"\").replace(\"*\",\"\").replace(\"'\",\"\").replace(\"{\",\"\").replace(\"}\",\"\") \n",
    "            except:\n",
    "                whatt = str(arr[14])\n",
    "\n",
    "            #print(arr)\n",
    "            insertsql = ('INSERT INTO full_data VALUES'+(\"('\" +str(arr[0])+ \"','\" +str(arr[1])+ \"','\" +str(title)+ \"','\" +str(descrip)+\"','\" +summary+\"','\" +str((', '.join(arr[5])))+\"','\" +str(arr[6])+\"','\" +str(len(arr[7]))+\"','\" + str(arr[8]) +\"','\" + str(arr[9]) +\"','\" +str(arr[10])+\"','\" +str(arr[11])+\"','\" +str(arr[12])+\"','\" +str(arr[13])+\"','\" +str(whatt)+ \"','\" +str(arr[15])+\"','\" +str(arr[16])+\"','\" +cite+\"','\" +str(arr[18])+\"','\" +source+\"','\" +str(arr[20])+\"','\" +str(arr[21])+\"','\" +str(arr[23])+\"','\" +str(arr[24])+\"','\" +str(arr[25])+\"','\" +str(arr[26])+\"','\" +str(arr[27])+ \"')\" )) \n",
    "            #print (insertsql)\n",
    "            cursorObj.execute(insertsql)\n",
    "            con.commit()\n",
    "            barposition+=1\n",
    "            bar.update(i)\n",
    "\n",
    "def summary(con):\n",
    "    cursorObj = con.cursor()\n",
    "\n",
    "    today = date.today()\n",
    "    d1 = today.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    cursorObj = con.cursor()\n",
    "    names_query = \"\"\"SELECT Name from full_data\"\"\"\n",
    "    cursorObj.execute(names_query)\n",
    "    names_list = cursorObj.fetchall()\n",
    "\n",
    "    names_query = \"\"\"SELECT files from full_data\"\"\"\n",
    "    cursorObj.execute(names_query)\n",
    "    files_list = cursorObj.fetchall()\n",
    "    totalfiles = 0\n",
    "    files_list = list(files_list)\n",
    "    for i in files_list:\n",
    "        num = int(i[0])\n",
    "        totalfiles = totalfiles+num\n",
    "\n",
    "    names_query = \"\"\"SELECT Collection from full_data\"\"\"\n",
    "    cursorObj.execute(names_query)\n",
    "    collection_list = cursorObj.fetchall()\n",
    "    collection_list = list(set(collection_list))\n",
    "    for i in range (len(collection_list)-1):\n",
    "        if collection_list[i][0] == \"nan\":\n",
    "            del collection_list[i]\n",
    "    len(collection_list)\n",
    "\n",
    "    topics_query = \"\"\"SELECT Topic from full_data\"\"\"\n",
    "    cursorObj.execute(topics_query)\n",
    "    topic_list = cursorObj.fetchall()\n",
    "    topic_list = list(set(topic_list))\n",
    "    for i in range (len(topic_list)-1):\n",
    "        if topic_list[i][0] == \"nan\":\n",
    "            del topic_list[i]\n",
    "    len(topic_list)\n",
    "\n",
    "    tags_query = \"\"\"SELECT tag from dftags\"\"\"\n",
    "    cursorObj.execute(tags_query)\n",
    "    tags_list = cursorObj.fetchall()\n",
    "    tags_list = list(set(tags_list))\n",
    "    for i in range (len(tags_list)-1):\n",
    "        if tags_list[i][0] == \"nan\":\n",
    "            del tags_list[i]\n",
    "    len(tags_list)\n",
    "\n",
    "    countries_query = \"\"\"SELECT country from countries\"\"\"\n",
    "    cursorObj.execute(countries_query)\n",
    "    countries_list = cursorObj.fetchall()\n",
    "    countries_list = list(set(countries_list))\n",
    "    for i in range (len(countries_list)-1):\n",
    "        if countries_list[i][0] == \"nan\":\n",
    "            del countries_list[i]\n",
    "    len(countries_list)\n",
    "\n",
    "    minD_query = \"\"\"SELECT date from dates\"\"\"\n",
    "    cursorObj.execute(minD_query)\n",
    "    minD_list = cursorObj.fetchall()\n",
    "    minD_list = list(set(minD_list))\n",
    "    mindnum = 2020\n",
    "    for i in range (len(minD_list)):\n",
    "        if minD_list[i][0] == \"nan\":\n",
    "            pass\n",
    "        else:\n",
    "            num= int(float(minD_list[i][0]))\n",
    "            #print (num)\n",
    "            if num < mindnum:\n",
    "                mindnum = num\n",
    "    \n",
    "    cursorObj.execute('DELETE FROM summary;',);\n",
    "    con.commit()\n",
    "    \n",
    "    insertsql = ('INSERT INTO summary VALUES'+(\"('\" +d1+ \"','\" +str(len(names_list))+ \"','\" +str(totalfiles)+ \"','\" +str(len(collection_list))+\"','\" +str(len(topic_list))+\"','\" +str(len(tags_list))+\"','\" +str(len(countries_list))+\"','\" +str(mindnum) + \"')\" )) \n",
    "    print (insertsql)\n",
    "    cursorObj.execute(insertsql)\n",
    "    print ('Resume updated to the SQL')\n",
    "    con.commit()\n",
    "    \n",
    "def insertDFdates(df, con):\n",
    "    cursorObj = con.cursor()\n",
    "    cursorObj.execute('DELETE FROM dates;',);\n",
    "    con.commit()\n",
    "    barposition=0\n",
    "    with progressbar.ProgressBar(max_value=len(df)) as bar:\n",
    "        cursorObj = con.cursor()\n",
    "        for i,row in df.iterrows():\n",
    "            arr = df.iloc[i]\n",
    "            insertsql = ('INSERT INTO dates VALUES'+(\"('\"+str(arr[0])+\"' , '\"+str(arr[1])+\"')\"))\n",
    "            cursorObj.execute(insertsql)\n",
    "            con.commit()\n",
    "            barposition+=1\n",
    "            bar.update(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertDFcountries(df, con):\n",
    "    cursorObj = con.cursor()\n",
    "    cursorObj.execute('DELETE FROM countries;',);\n",
    "    con.commit()\n",
    "\n",
    "    barposition=0\n",
    "    with progressbar.ProgressBar(max_value=len(df)) as bar:\n",
    "        cursorObj = con.cursor()\n",
    "        for i,row in df.iterrows():\n",
    "            arr = df.iloc[i]\n",
    "            country = str(arr[1]).replace(\"'\",\"\").replace(\"Â´\",\"\")\n",
    "            insertsql = ('INSERT INTO countries VALUES'+(\"('\"+str(arr[0])+\"' , '\"+country+\"')\"))\n",
    "            cursorObj.execute(insertsql)\n",
    "            con.commit()\n",
    "            barposition+=1\n",
    "            bar.update(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the conection to the db and create the cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = pymysql.connect(host='77.240.114.226',\n",
    "                     user='wpp_challenge',\n",
    "                     password='Qmi2b3?2',\n",
    "                     db='wpp_challenge')\n",
    "cursorObj = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the tables, if it was created before, don't do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the table for the dftags.\n",
    "try: \n",
    "    sql_tabletag(con)\n",
    "    print ('Table TAG created')    \n",
    "except: \n",
    "    print ('Table TAG already exist')\n",
    "\n",
    "#Create the table for the dftags.\n",
    "try: \n",
    "    sql_tablefull(con)\n",
    "    print ('Table fulldata  created')\n",
    "\n",
    "except: \n",
    "    print ('Table FULL DATA already exist')\n",
    "\n",
    "#Create the table for the resume.\n",
    "try: \n",
    "    sql_tablesummary(con)\n",
    "    print ('Table RESUME  created')\n",
    "\n",
    "except: \n",
    "    print ('Table RESUME already exist')\n",
    "    \n",
    "try: \n",
    "    sql_tabledates(con)\n",
    "    print ('Table DATES  created')\n",
    "\n",
    "except: \n",
    "    print ('Table DATES already exist')\n",
    "\n",
    "try: \n",
    "    sql_tablecountries(con)\n",
    "    print ('Table COUNTRIES  created')\n",
    "\n",
    "except: \n",
    "    print ('Table COUNTRIES already exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we use the data from the different df's to upload our tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertDFtags(dftags, con)\n",
    "insertDFfull(dfF, con)\n",
    "insertDFdates(dfdates, con)\n",
    "insertDFcountries(dfcountries, con)\n",
    "summary(con)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
